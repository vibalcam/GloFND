{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir('..')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from sklearn.manifold import TSNE\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "# load package\n",
    "import matplotlib as mpl\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.ticker import MultipleLocator,FormatStrFormatter,MaxNLocator\n",
    "from tqdm.auto import tqdm\n",
    "import argparse\n",
    "import builtins\n",
    "from datetime import datetime\n",
    "import math\n",
    "import os\n",
    "import random\n",
    "import shutil\n",
    "import wandb\n",
    "import time\n",
    "import warnings\n",
    "import numpy as np\n",
    "from functools import partial\n",
    "from utils import utils\n",
    "# from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.parallel\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.distributed as dist\n",
    "import torch.optim\n",
    "import torch.multiprocessing as mp\n",
    "import torch.utils.data\n",
    "import torch.utils.data.distributed\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as datasets\n",
    "from torchvision import models as torchvision_models\n",
    "# import torchvision.models as torchvision_models\n",
    "# from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "import sogclr.builder\n",
    "import sogclr.loader\n",
    "import sogclr.optimizer\n",
    "import sogclr.folder # imagenet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.set_device(9)\n",
    "model_paths = [\n",
    "    \"baselines/tuning_20241028_153626_dcl_0_0_0_0_-1_0/3/checkpoint_0199.pth.tar\",\n",
    "]\n",
    "data = \"/data/datasets/imagenet100\"\n",
    "arch = \"resnet50\"\n",
    "batch_size = 2048"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data loading code\n",
    "mean = [0.485, 0.456, 0.406]\n",
    "std = [0.229, 0.224, 0.225]\n",
    "\n",
    "image_size = 224\n",
    "normalize = transforms.Normalize(mean=mean, std=std)\n",
    "\n",
    "augmentation1 = transforms.Compose([\n",
    "    transforms.RandomResizedCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "    normalize,\n",
    "])\n",
    "\n",
    "traindir = os.path.join(data, 'train')\n",
    "train_dataset = sogclr.folder.ImageFolder(\n",
    "    traindir,\n",
    "    augmentation1)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    train_dataset, batch_size=batch_size, shuffle=False,\n",
    "    num_workers=6, pin_memory=True, drop_last=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _build_mlp(num_layers, input_dim, mlp_dim, output_dim, last_bn=True):\n",
    "    mlp = []\n",
    "    for l in range(num_layers):\n",
    "        dim1 = input_dim if l == 0 else mlp_dim\n",
    "        dim2 = output_dim if l == num_layers - 1 else mlp_dim\n",
    "\n",
    "        mlp.append(nn.Linear(dim1, dim2, bias=False))\n",
    "\n",
    "        if l < num_layers - 1:\n",
    "            mlp.append(nn.BatchNorm1d(dim2))\n",
    "            mlp.append(nn.ReLU(inplace=True))\n",
    "        elif last_bn:\n",
    "            # follow SimCLR's design: https://github.com/google-research/simclr/blob/master/model_util.py#L157\n",
    "            # for simplicity, we further removed gamma in BN\n",
    "            mlp.append(nn.BatchNorm1d(dim2, affine=False))\n",
    "\n",
    "    return nn.Sequential(*mlp)\n",
    "\n",
    "models = []\n",
    "for model_path in model_paths:\n",
    "    model = torchvision_models.__dict__[arch](pretrained=False)\n",
    "    # Remove the final fully connected layer and add an identity layer\n",
    "    if hasattr(model, 'fc'):\n",
    "        linear_keyword = 'fc'\n",
    "        hidden_dim = model.fc.weight.shape[1]\n",
    "        # model.fc = nn.Identity()\n",
    "        model.fc = _build_mlp(2, hidden_dim, 2048, 128)\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported model architecture: {arch}\")\n",
    "\n",
    "    # Load checkpoint\n",
    "    if os.path.isfile(model_path):\n",
    "        print(f\"=> Loading checkpoint '{model_path}'\")\n",
    "        checkpoint = torch.load(model_path, map_location=\"cpu\")\n",
    "        state_dict = checkpoint.get('state_dict', checkpoint)\n",
    "        lda = state_dict['module.lambda_threshold.lda']\n",
    "        # get base_encoder state_dict\n",
    "        state_dict = {k.replace('module.base_encoder.', ''): v for k, v in state_dict.items() if 'module.base_encoder.' in k}\n",
    "        msg = model.load_state_dict(state_dict, strict=True)\n",
    "        print(f\"=> Loaded checkpoint with missing keys: {msg.missing_keys}\")\n",
    "        # assert all([linear_keyword in k for k in msg.missing_keys]), \"Missing keys should be linear layers\"\n",
    "        model.eval()\n",
    "    else:\n",
    "        print(f\"Warning: Checkpoint {model_path} not found!\")\n",
    "        raise FileNotFoundError\n",
    "\n",
    "    models.append(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.auto import tqdm\n",
    "\n",
    "h_list = []\n",
    "i_list = []\n",
    "for model in models:\n",
    "    hidden_list1 = []\n",
    "    indices = []\n",
    "\n",
    "    with torch.inference_mode():\n",
    "        model.cuda()\n",
    "\n",
    "        tqdm_progress = tqdm(total=len(train_loader), leave=False)\n",
    "        for images, labels, index in train_loader:\n",
    "            images = images.cuda(non_blocking=True)\n",
    "\n",
    "            # compute output\n",
    "            with torch.cuda.amp.autocast(True):\n",
    "                hidden1 = model(images)\n",
    "            hidden1 = F.normalize(hidden1, p=2, dim=1)\n",
    "            hidden_list1.append(hidden1.cpu())\n",
    "            indices.append(index.cpu())\n",
    "\n",
    "            tqdm_progress.update(images.shape[0])\n",
    "\n",
    "        model.cpu()\n",
    "\n",
    "    h_list.append(torch.cat(hidden_list1, dim=0))\n",
    "    i_list.append(torch.cat(indices, dim=0))\n",
    "\n",
    "h_list = torch.stack(h_list, dim=0)\n",
    "i_list = torch.stack(i_list, dim=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot(projections, meta, kl, alpha):\n",
    "    # Get unique labels\n",
    "    unique_labels = np.unique(meta)\n",
    "\n",
    "    # Plot the projections with different colors and symbols for each label\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    # plt.title(f\"KL divergence: {kl:0.2f}\", fontsize=60)\n",
    "\n",
    "    cmap = plt.cm.get_cmap(\"jet\", len(unique_labels))\n",
    "    # cmap = generate_colormap(len(unique_labels))\n",
    "\n",
    "    for idx,label in enumerate(unique_labels):\n",
    "        mask = (meta == label)\n",
    "        \n",
    "        # Use a different color and marker for each label\n",
    "        # color = plt.cm.jet(idx / float(len(unique_labels)))\n",
    "        color = cmap(idx / float(len(unique_labels)))\n",
    "        # marker = 'o' if label in BIRD_LABELS else '^'\n",
    "        marker = 'o'\n",
    "        \n",
    "        plt.scatter(\n",
    "            projections[mask, 0], \n",
    "            projections[mask, 1], \n",
    "            color=[color], \n",
    "            marker=marker, \n",
    "            label=f'Label {label}', \n",
    "            alpha=0.8,\n",
    "            s=25,\n",
    "        )\n",
    "    plt.axis('off')  # Turn off the axes\n",
    "    plt.tight_layout()\n",
    "\n",
    "    # plt.show()\n",
    "    plt.savefig(f\"results/embeddings/alpha_{alpha}.png\", format=\"png\", dpi=300)\n",
    "    plt.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "def get_pth_files(folder_path):\n",
    "    # Get all .pth files in the folder\n",
    "    folder = Path(folder_path)\n",
    "    pth_files = list(folder.glob(\"*.pth\"))\n",
    "    return pth_files\n",
    "\n",
    "# Example usage\n",
    "folder = \"results/embeddings\"\n",
    "pth_files = get_pth_files(folder)\n",
    "print(pth_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pth_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for f in pth_files:\n",
    "    data = torch.load(f)\n",
    "    plot(data[\"proj\"], data[\"meta\"].numpy(), data[\"kl\"], data[\"name\"])\n",
    "    # plot(p, m.numpy(), k, n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
