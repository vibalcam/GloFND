{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "import os\n",
    "from utils.utils import load_json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import ttest_rel\n",
    "\n",
    "def calculate_means(list_folder_paths, baseline_name=None, output_format=\"pandas\", num_runs=3, args_print=[]):\n",
    "    # Dictionary to store results by experiment name\n",
    "    # experiment_results = defaultdict(list)\n",
    "    experiment_results = defaultdict(lambda: defaultdict(lambda: ([], [], [])))\n",
    "\n",
    "    # Traverse the folder and find lincls.json files\n",
    "    for folder_path in list_folder_paths:\n",
    "        for root, _, files in os.walk(folder_path):\n",
    "            for file in files:\n",
    "                if file == \"lincls.json\":\n",
    "                    # Extract experiment name and number from folder structure\n",
    "                    parts = os.path.relpath(root, folder_path).split(os.sep)\n",
    "                    if len(parts) >= 2:\n",
    "                        experiment_name = parts[0]\n",
    "                        # Load the json file\n",
    "                        file_path = os.path.join(root, file)\n",
    "                        data = load_json(file_path)\n",
    "                        for d in data:\n",
    "                            curr_args = d[\"args\"]\n",
    "                            if os.path.exists(os.path.join(root, \"config.json\")):\n",
    "                                curr_args.update(load_json(os.path.join(root, \"config.json\")))\n",
    "                            # else:\n",
    "                            #     print(f\"Warning: No config.json found in {root}\")\n",
    "                            experiment_results[experiment_name][d[\"args\"][\"sample_images\"]][0].append(curr_args)\n",
    "                            experiment_results[experiment_name][d[\"args\"][\"sample_images\"]][1].append(d[\"best_acc1\"])\n",
    "                            experiment_results[experiment_name][d[\"args\"][\"sample_images\"]][2].append(d[\"best_acc5\"])\n",
    "\n",
    "    # Calculate means for each experiment name\n",
    "    mean_results = {}\n",
    "    baseline_results = experiment_results.get(baseline_name, None)\n",
    "    for name, results in experiment_results.items():\n",
    "        for k,v in results.items():\n",
    "            # print(k)\n",
    "            item_key = f\"{name}_{k}\"\n",
    "            assert len(v[0]) == num_runs, f\"Expected {num_runs} runs, but got {len(v[0])} on {item_key}\"\n",
    "            assert float(v[0][0][\"sample_images\"]) == float(k), f\"Expected {k} sample images, but got {v[0][0]['sample_images']}\"\n",
    "            mean_results[item_key] = [item_key, name, float(v[0][0][\"sample_images\"])]\n",
    "            # mean_results[item_key] += [v[0][0][i] if i in v[0][0] else None for i in args_print]\n",
    "            mean_results[item_key] += [v[0][0].get(i, None) for i in args_print]\n",
    "            mean_results[item_key] += [\n",
    "                # mean acc1 and std acc1\n",
    "                np.mean(v[1]),\n",
    "                np.std(v[1]),\n",
    "                # mean acc5 and std acc5\n",
    "                np.mean(v[2]),\n",
    "                np.std(v[2]),\n",
    "            ]\n",
    "\n",
    "            # Compute p-values if the baseline exists for the same sample_images\n",
    "            if baseline_results is not None:\n",
    "                baseline_acc1 = baseline_results[k][1]\n",
    "                baseline_acc5 = baseline_results[k][2]\n",
    "                assert len(baseline_acc1) == len(v[1]), f\"Expected {len(v[1])} runs, but got {len(baseline_acc1)}\"\n",
    "                assert len(baseline_acc5) == len(v[2]), f\"Expected {len(v[2])} runs, but got {len(baseline_acc5)}\"\n",
    "                p_value_acc1 = round(ttest_rel(baseline_acc1, v[1], nan_policy='raise', alternative='less')[1], 3)\n",
    "                p_value_acc5 = round(ttest_rel(baseline_acc5, v[2], nan_policy='raise', alternative='less')[1], 3)\n",
    "                mean_results[item_key].extend([p_value_acc1, p_value_acc5])\n",
    "\n",
    "            else:\n",
    "                mean_results[item_key].extend([None, None])\n",
    "    \n",
    "    # Create a DataFrame with values\n",
    "    df = pd.DataFrame(mean_results.values(), columns=[\"key\", \"name\", \"sample_images\"] + args_print + [\"mean_acc1\", \"std_acc1\", \"mean_acc5\", \"std_acc5\", \"pvalue_acc1\", \"pvalue_acc5\"])\n",
    "\n",
    "    # Output based on the specified format\n",
    "    if output_format.lower() == \"markdown\":\n",
    "        print(\"### Results in Markdown\")\n",
    "        print(df.to_markdown(index=False))\n",
    "    elif output_format.lower() == \"latex\":\n",
    "        print(\"### Results in LaTeX\")\n",
    "        print(df.to_latex(index=False, caption=\"Experiment Results\", label=\"tab:experiment_results\"))\n",
    "    else:  # Default is pandas\n",
    "        print(\"### Results as Pandas DataFrame\")\n",
    "        # print(df)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_folder_paths = [\"results/eval_saved_dcl\",\"baselines\"]\n",
    "output_format = \"pandas\"\n",
    "baseline = \"tuning_20241028_153626_dcl_0_0_0_0_-1_0\"\n",
    "args_print = [\n",
    "    \"loss_type\",\n",
    "    \"glofnd\",\n",
    "    \"alpha\",\n",
    "    \"start_update\",\n",
    "    \"lr_lda\",\n",
    "]\n",
    "sort_order = [\n",
    "    'sign_acc1', 'pvalue_acc1', 'mean_acc1', 'std_acc1'] + args_print\n",
    "df = calculate_means(list_folder_paths, baseline_name=baseline, output_format=output_format, args_print=args_print)\n",
    "df[\"sign_acc1\"] = df[\"pvalue_acc1\"] < 0.05"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_colwidth', None)\n",
    "df[['mean_acc1', 'std_acc1']] = df[['mean_acc1', 'std_acc1']].round(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df) / 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df.sample_images == 1].sort_values(by=\"mean_acc1\", ascending=False)[sort_order]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df.sample_images == 0.1].sort_values(by=\"mean_acc1\", ascending=False)[sort_order]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df.sample_images == 0.01].sort_values(by=\"mean_acc1\", ascending=False)[sort_order]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df.sample_images == 0.001].sort_values(by=\"mean_acc1\", ascending=False)[sort_order]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fastclip",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
